# AI Mental Health Companions and Therapeutic Alliance

## Overview

Recent research has examined whether AI chatbots can form therapeutic relationships with users and whether these relationships are beneficial. The findings are surprisingly positive, with important caveats.

## Key Clinical Trials

### Therabot RCT (2024/2025)

**Published in:** NEJM AI (New England Journal of Medicine)
**Source:** [NEJM AI Study](https://ai.nejm.org/doi/full/10.1056/AIoa2400802)

This is the **first randomized controlled trial** demonstrating effectiveness of a fully generative AI therapy chatbot.

**Study details:**

- 210 adults with clinically significant symptoms
- Conditions: Major Depressive Disorder (MDD), Generalized Anxiety Disorder (GAD), or high risk for eating disorders
- 4-week intervention vs. waitlist control

**Key findings:**

- Average usage: >6 hours over the trial
- Participants rated therapeutic alliance **comparable to human therapists**
- Significant improvement in mental health symptoms

**Surprising finding:**

> "We did not expect that people would almost treat the software like a friend. It says to me that they were actually forming relationships with Therabot."

## Digital Therapeutic Alliance (DTA)

### Traditional Therapeutic Alliance (Bordin)

Edward Bordin defined therapeutic alliance as having three components:

1. Agreeing on therapeutic goals
2. Assigning therapeutic tasks
3. Developing therapeutic bonds

### Adapting for AI

Since AI cannot form "genuine" relationships, researchers propose a **"digital therapeutic alliance"** (DTA) - a user-perceived alliance where users:

- Agree on tasks toward therapeutic goals
- Feel a sense of connection with the chatbot
- Trust the system enough to engage

### Research on Bonding

A diary study found:

- 18 participants reported forming a "bond" or "light bond" with at least one chatbot
- Three categories emerged: Bond, Light Bond, No Bond
- **Both higher and lower well-being participants** formed bonds
- Bonding capacity wasn't dependent on mental health status

## Effectiveness of AI Mental Health Chatbots

### Systematic Review Findings

Review of popular chatbots (Woebot, Wysa, Youper):

**Woebot:**

- Remarkable reductions in depression and anxiety
- High user engagement

**Youper:**

- 48% decrease in depression symptoms
- 43% decrease in anxiety symptoms

**Common benefits across all:**

- Therapeutic alliance formation
- High user satisfaction rates

### Comparison to Human Therapists

One study compared responses from human therapists vs. LLM-based chatbots:

- AI responses were sometimes rated as more empathetic
- Humans excelled at nonverbal cues (unavailable to AI)
- Alliance ratings were comparable in some dimensions

## Concerns and Limitations

### Therapeutic Misconception

Users may:

- Overestimate AI's ability to provide therapeutic support
- Underestimate its restrictions
- Form inappropriate reliance on the technology

### Sources of Misconception

- Inaccurate marketing
- Strong digital therapeutic alliance
- Inadequate design leading to biases
- Potential limitation of user autonomy

### Technology Limitations

1. Cannot analyze nonverbal cues or tone of voice
2. Generic conversation styles may not make users feel understood
3. High validation without genuine understanding

### AI Bias Concerns

Stanford research found AI chatbots showed **increased stigma** toward certain conditions (alcohol dependence, schizophrenia) compared to others (depression). This could harm patients and lead them to discontinue care.

## Implications for Our Project

### What We Can Learn

**Positive findings:**

- Users CAN form meaningful perceived relationships with AI
- Therapeutic alliance with AI is possible
- High engagement is achievable (>6 hours in 4 weeks)
- Mental health benefits can occur

**Cautions:**

- Don't claim to be therapy or therapist
- Don't overclaim effectiveness
- Be honest about limitations
- Avoid creating inappropriate dependency

### Design Principles

1. **Authenticity** - Don't pretend to be human or have emotions
2. **Bounded claims** - Be clear about what the companion can and can't do
3. **Complement, not replace** - Position as support alongside other strategies
4. **Monitor for misconception** - Help users maintain realistic expectations

### Specific to Body Doubling (vs. Therapy)

Our use case is different from therapy chatbots:

- Not treating clinical conditions
- Focus on productivity, not mental health treatment
- Accountability relationship, not therapeutic relationship
- Task-focused, not emotion-focused

But we can still learn from:

- How users form relationships with AI
- Importance of consistency and memory
- Value of perceived understanding

## Key Sources

### Clinical Research

- [NEJM AI: Therabot RCT](https://ai.nejm.org/doi/full/10.1056/AIoa2400802)
- [Dartmouth News: Therapy Chatbot Trial](https://home.dartmouth.edu/news/2025/03/first-therapy-chatbot-trial-yields-mental-health-benefits)
- [PMC: AI-Powered Mental Health Chatbots](https://pmc.ncbi.nlm.nih.gov/articles/PMC10663264/)

### Reviews and Meta-Analyses

- [Frontiers: Robot Therapist Understanding](https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2023.1278186/full)
- [PMC: CBT Chatbots Systematic Review](https://pmc.ncbi.nlm.nih.gov/articles/PMC11904749/)
- [MDPI: AI Chatbots for Mental Health Scoping Review](https://www.mdpi.com/2076-3417/14/13/5889)

### Specific Studies

- [Nature: Experiences of Gen AI Chatbots for Mental Health](https://www.nature.com/articles/s44184-024-00097-4)
- [ScienceDirect: Digital Therapeutic Alliance Diary Study](https://www.sciencedirect.com/org/science/article/pii/S2368795925001039)
- [JMIR: Human vs LLM Therapeutic Communication](https://mental.jmir.org/2025/1/e69709)
- [Stanford HAI: Dangers of AI in Mental Health](https://hai.stanford.edu/news/exploring-the-dangers-of-ai-in-mental-health-care)
